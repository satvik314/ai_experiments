{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMISIieQOMCWNCrQatFus7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik314/ai_experiments/blob/main/langraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UQkg2rWNqhmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fec3d1-d497-450e-dba9-0beacd016251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -qU langgraph langchain langchain_openai tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass(\"openai: \")\n",
        "os.environ['TAVILY_API_KEY'] = getpass(\"tavily: \")\n",
        "os.environ['LANGCHAIN_API_KEY'] = getpass(\"langchain: \")\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = \"true\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9bXPl10sPZc",
        "outputId": "68be61f6-ac17-40fa-bae9-ffeaa2f2c3fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "openai: ··········\n",
            "tavily: ··········\n",
            "langchain: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup the tools\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "tools = [TavilySearchResults(max_results=1)]\n",
        "\n",
        "#wrap the tools in tool_executor\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "tool_executor = ToolExecutor(tools)\n"
      ],
      "metadata": {
        "id": "Z65kLLkdsUYq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up model and functions\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "\n",
        "model = ChatOpenAI(temperature = 0, streaming = True)\n",
        "\n",
        "functions = [format_tool_to_openai_function(t) for t in tools]\n",
        "\n",
        "#binding functions to the model\n",
        "model = model.bind_functions(functions)\n"
      ],
      "metadata": {
        "id": "Vqnw8KUqsUPE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define agent state\n",
        "\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "#every agent state is a list of messages\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ],
      "metadata": {
        "id": "preX7JlGum0m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the nodes\n",
        "\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "#define a function whether to continue or not\n",
        "def should_continue(state):\n",
        "  messages = state['messages']\n",
        "  last_message = messages[-1]\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\"\n",
        "\n",
        "#define the function that calls the model\n",
        "def call_model(state):\n",
        "  messages = state['messages']\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "#define the functions to execute tools\n",
        "def call_tool(state):\n",
        "  messages = state['messages']\n",
        "  last_message = messages[-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool = last_message.additional_kwargs['function_call']['name'],\n",
        "      tool_input = json.loads(last_message.additional_kwargs['function_call']['arguments'])\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "  function_message = FunctionMessage(content = str(response), name = action.tool)\n",
        "  return {\"messages\" : [function_message]}"
      ],
      "metadata": {
        "id": "XwFoS9m6wLsc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    # First, we define the start node. We use `agent`.\n",
        "    # This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_continue,\n",
        "    # Finally we pass in a mapping.\n",
        "    # The keys are strings, and the values are other nodes.\n",
        "    # END is a special node marking that the graph should finish.\n",
        "    # What will happen is we will call `should_continue`, and then the output of that\n",
        "    # will be matched against the keys in this mapping.\n",
        "    # Based on which one it matches, that node will then be called.\n",
        "    {\n",
        "        # If `tools`, then we call the tool node.\n",
        "        \"continue\": \"action\",\n",
        "        # Otherwise we finish.\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that after `tools` is called, `agent` node is called next.\n",
        "workflow.add_edge('action', 'agent')\n",
        "\n",
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "q3q4lMvFyrAN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content = \"what is the weather in Koramangala?\")]}\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRZ1V4DvzvVP",
        "outputId": "16bfb898-1e08-4608-853d-42c041d36943"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the weather in Koramangala?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"weather in Koramangala\"\\n}', 'name': 'tavily_search_results_json'}}),\n",
              "  FunctionMessage(content=\"[{'url': 'https://weather.com/en-IN/weather/tenday/l/Bangalore+Karnataka?canonicalCityId=73dd3db2099ff26d4daa3c9346a09dd27d681b36ce0b94b63f14ace60adc209e', 'content': 'recents Special Forecasts 10-Day Weather-Koramangala 5th Block, Karnataka Today Fri 26 | Day  Today Fri 26 | Day Partly cloudy. High 29°C. Winds E at 10 to 15 km/h. Fri 26 | Night  Fri 26 | Night Generally clear. Hazy. Low 17°C. Winds E at 10 to 15 km/h. Sat 27 Sat 27 | Day  Thu 01 Thu 01 | Day Partly cloudy. High 31°C. Winds SSW and variable. Thu 01 | Night10-Day Weather - Koramangala 5th Block, Karnataka As of 17:11 IST Tonight --/ 16° 3% | Night 16° 3% E 12 km/h Generally clear. Hazy. Low 16°C. Winds E at 10 to 15 km/h. Humidity 69% UV Index 0...'}]\", name='tavily_search_results_json'),\n",
              "  AIMessage(content='The weather in Koramangala is currently partly cloudy with a high of 29°C. The wind is coming from the east at 10 to 15 km/h. Tonight, it will be generally clear and hazy with a low of 17°C. The wind will continue to come from the east at 10 to 15 km/h.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rg438xaI0Hio"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}