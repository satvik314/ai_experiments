{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNu+q1x3xpanavYuagJDphV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik314/ai_experiments/blob/main/ft_axolotl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "___f4MtOpZcF",
        "outputId": "9926ffdb-83f3-4e65-b050-43cf97a89c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'axolotl'...\n",
            "remote: Enumerating objects: 9357, done.\u001b[K\n",
            "remote: Counting objects: 100% (2440/2440), done.\u001b[K\n",
            "remote: Compressing objects: 100% (470/470), done.\u001b[K\n",
            "remote: Total 9357 (delta 2197), reused 2073 (delta 1939), pack-reused 6917\u001b[K\n",
            "Receiving objects: 100% (9357/9357), 3.19 MiB | 4.88 MiB/s, done.\n",
            "Resolving deltas: 100% (6097/6097), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfvoplFTq779",
        "outputId": "6d2b4681-e6fb-49f9-ffa6-407a0e69f7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZ5yZggrntt",
        "outputId": "577c296b-0aa5-41cf-c2a1-c4d62cddc32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e '.[flash-attn, deepspeed]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhvqXgENrrX9",
        "outputId": "0d72137d-2e45-4da1-bc1d-98453faa5d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content\n",
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk2elVfkr6lV",
        "outputId": "33f330b9-608e-4301-dc94-d1217fdb558b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "axolotl  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd axolotl/"
      ],
      "metadata": {
        "id": "mea5Mlf1tFcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/axolotl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxILsm-btJok",
        "outputId": "e2768ce0-1042-4f14-9b68-6192af0a7eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/axolotl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8LEepBptfpV",
        "outputId": "0dd3cbc4-9646-4178-8524-0408e023a840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/axolotl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e '.[flash-attn, deepspeed]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y6vZy1anthEC",
        "outputId": "ca22307e-e21d-437e-bc97-889abd0bea6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/axolotl\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging==23.2 in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (23.2)\n",
            "Collecting peft==0.7.0 (from axolotl==0.3.0)\n",
            "  Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.37.0 (from axolotl==0.3.0)\n",
            "  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers==0.15.0 in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (0.15.0)\n",
            "Collecting bitsandbytes>=0.41.1 (from axolotl==0.3.0)\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.26.1 (from axolotl==0.3.0)\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from axolotl==0.3.0)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting fire (from axolotl==0.3.0)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (6.0.1)\n",
            "Collecting datasets>=2.15.0 (from axolotl==0.3.0)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from axolotl==0.3.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from axolotl==0.3.0)\n",
            "  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from axolotl==0.3.0)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers==0.0.22 (from axolotl==0.3.0)\n",
            "  Downloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl (211.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum==1.13.2 (from axolotl==0.3.0)\n",
            "  Downloading optimum-1.13.2.tar.gz (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.0/301.0 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hf_transfer (from axolotl==0.3.0)\n",
            "  Downloading hf_transfer-0.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama (from axolotl==0.3.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (0.58.1)\n",
            "Collecting numpy>=1.24.4 (from axolotl==0.3.0)\n",
            "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlflow (from axolotl==0.3.0)\n",
            "  Downloading mlflow-2.9.2-py3-none-any.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score==0.3.13 (from axolotl==0.3.0)\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.4.0 (from axolotl==0.3.0)\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score==0.1.2 (from axolotl==0.3.0)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (1.2.2)\n",
            "Collecting pynvml (from axolotl==0.3.0)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting art (from axolotl==0.3.0)\n",
            "  Downloading art-6.1-py3-none-any.whl (599 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.8/599.8 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fschat==0.2.34 (from axolotl==0.3.0)\n",
            "  Downloading fschat-0.2.34-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio==3.50.2 (from axolotl==0.3.0)\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (2.15.1)\n",
            "Collecting s3fs (from axolotl==0.3.0)\n",
            "  Downloading s3fs-2023.12.2-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (from axolotl==0.3.0) (2023.6.0)\n",
            "Collecting trl>=0.7.9 (from axolotl==0.3.0)\n",
            "  Downloading trl-0.7.10-py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flash-attn==2.3.3 (from axolotl==0.3.0)\n",
            "  Downloading flash_attn-2.3.3.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deepspeed (from axolotl==0.3.0)\n",
            "  Downloading deepspeed-0.13.0.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1->axolotl==0.3.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1->axolotl==0.3.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1->axolotl==0.3.0) (0.20.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1->axolotl==0.3.0) (0.4.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.3.0) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.3.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.3.0) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.3.0) (3.7.1)\n",
            "Collecting dill (from evaluate==0.4.0->axolotl==0.3.0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->axolotl==0.3.0) (3.4.1)\n",
            "Collecting multiprocess (from evaluate==0.4.0->axolotl==0.3.0)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->axolotl==0.3.0) (2023.6.0)\n",
            "Collecting responses<0.19 (from evaluate==0.4.0->axolotl==0.3.0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting ninja (from flash-attn==2.3.3->axolotl==0.3.0)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.34->axolotl==0.3.0) (3.9.1)\n",
            "Collecting fastapi (from fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown2[all] (from fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading markdown2-2.4.12-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nh3 (from fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.34->axolotl==0.3.0) (3.0.43)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.34->axolotl==0.3.0) (1.10.13)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.34->axolotl==0.3.0) (13.7.0)\n",
            "Collecting shortuuid (from fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting tiktoken (from fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading uvicorn-0.27.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl==0.3.0) (4.2.2)\n",
            "Collecting ffmpy (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl==0.3.0) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl==0.3.0) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl==0.3.0) (2.1.3)\n",
            "Collecting orjson~=3.0 (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading orjson-3.9.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl==0.3.0) (9.4.0)\n",
            "Collecting pydub (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl==0.3.0) (4.5.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from optimum==1.13.2->axolotl==0.3.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum==1.13.2->axolotl==0.3.0) (1.12)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum==1.13.2->axolotl==0.3.0) (4.35.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.3.0) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.3.0) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->axolotl==0.3.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->axolotl==0.3.0) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->axolotl==0.3.0) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->axolotl==0.3.0) (2023.6.3)\n",
            "Collecting torch>=1.10.0 (from accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0) (3.2.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch>=1.10.0->accelerate==0.26.1->axolotl==0.3.0)\n",
            "  Downloading lit-17.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.15.0->axolotl==0.3.0) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.15.0->axolotl==0.3.0) (0.6)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.7.9->axolotl==0.3.0)\n",
            "  Downloading tyro-0.6.6-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hjson (from deepspeed->axolotl==0.3.0)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->axolotl==0.3.0) (9.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->axolotl==0.3.0) (2.4.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->axolotl==0.3.0) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->axolotl==0.3.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs->axolotl==0.3.0) (1.2.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs->axolotl==0.3.0) (2.8.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (2.2.1)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow->axolotl==0.3.0)\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow->axolotl==0.3.0)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (2023.3.post1)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (7.0.1)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow->axolotl==0.3.0)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0 (from mlflow->axolotl==0.3.0)\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (2.2.5)\n",
            "Collecting querystring-parser<2 (from mlflow->axolotl==0.3.0)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (2.0.24)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow->axolotl==0.3.0) (3.5.2)\n",
            "Collecting gunicorn<22 (from mlflow->axolotl==0.3.0)\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->axolotl==0.3.0) (0.41.1)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->axolotl==0.3.0)\n",
            "  Downloading aiobotocore-2.11.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs (from axolotl==0.3.0)\n",
            "  Downloading s3fs-2023.12.1-py3-none-any.whl (28 kB)\n",
            "  Downloading s3fs-2023.10.0-py3-none-any.whl (28 kB)\n",
            "Collecting aiobotocore~=2.7.0 (from s3fs->axolotl==0.3.0)\n",
            "  Downloading aiobotocore-2.7.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3fs (from axolotl==0.3.0)\n",
            "  Downloading s3fs-2023.9.2-py3-none-any.whl (28 kB)\n",
            "Collecting aiobotocore~=2.5.4 (from s3fs->axolotl==0.3.0)\n",
            "  Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3fs (from axolotl==0.3.0)\n",
            "  Downloading s3fs-2023.9.1-py3-none-any.whl (28 kB)\n",
            "  Downloading s3fs-2023.9.0-py3-none-any.whl (28 kB)\n",
            "  Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl==0.3.0) (1.60.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl==0.3.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl==0.3.0) (3.0.1)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->axolotl==0.3.0)\n",
            "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->axolotl==0.3.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb->axolotl==0.3.0)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.3.0) (1.4.4)\n",
            "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.4->s3fs->axolotl==0.3.0)\n",
            "  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.4->s3fs->axolotl==0.3.0) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore~=2.5.4->s3fs->axolotl==0.3.0)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.34->axolotl==0.3.0) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.34->axolotl==0.3.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.34->axolotl==0.3.0) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.34->axolotl==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.34->axolotl==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.34->axolotl==0.3.0) (4.0.3)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow->axolotl==0.3.0)\n",
            "  Downloading Mako-1.3.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.3.0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.3.0) (0.12.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->axolotl==0.3.0) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->axolotl==0.3.0) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->axolotl==0.3.0) (0.9.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->axolotl==0.3.0) (2.0.7)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow->axolotl==0.3.0) (1.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow->axolotl==0.3.0) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow->axolotl==0.3.0)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->axolotl==0.3.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->axolotl==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->axolotl==0.3.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs->axolotl==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow->axolotl==0.3.0) (3.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat==0.2.34->axolotl==0.3.0) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.3.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.3.0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.3.0) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.34->axolotl==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.34->axolotl==0.3.0) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow->axolotl==0.3.0) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.7.9->axolotl==0.3.0)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.7.9->axolotl==0.3.0)\n",
            "  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.13.2->axolotl==0.3.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.36.0,>=0.35.0 (from fastapi->fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio==3.50.2->axolotl==0.3.0)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->axolotl==0.3.0) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->axolotl==0.3.0) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->axolotl==0.3.0) (2.7.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.34->axolotl==0.3.0) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.34->axolotl==0.3.0) (1.3.0)\n",
            "Collecting wavedrom (from markdown2[all]->fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum==1.13.2->axolotl==0.3.0) (1.3.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.4->s3fs->axolotl==0.3.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3<3,>=1.26.7 (from databricks-cli<1,>=0.8.7->mlflow->axolotl==0.3.0)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow->axolotl==0.3.0)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->axolotl==0.3.0) (1.62.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs->axolotl==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.3.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.3.0) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.3.0) (0.17.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.34->axolotl==0.3.0) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->axolotl==0.3.0) (0.5.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->fschat==0.2.34->axolotl==0.3.0) (1.2.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat==0.2.34->axolotl==0.3.0)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn, optimum, rouge-score, deepspeed, fire, ffmpy, wavedrom, lit\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.3.3-cp310-cp310-linux_x86_64.whl size=57042553 sha256=b1df92cb5bd7657d38b789dd48e907aa3e0bd2715c817eb85f3c4320bb11fb3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/e6/fa/941802ec61d1afd320d27160ab1db98e6dba65381f84b76d4a\n",
            "  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-1.13.2-py3-none-any.whl size=395599 sha256=919ba742336982bdb0e621a2d05f65c1d978ba44e3a916ba47aaa0e265a6d9f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/b7/2c/79405d98f0943373d8546daeae25a3d377f7659ca0cbe48699\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=aab87cebd0d9ca6d0e589f6138eadf71d500a3339f8561306906748ea216573b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.13.0-py3-none-any.whl size=1350531 sha256=1889baa82bcb03d7abc3b0d32b4ba0a1aa813a1c10b727c5a76921eb2e1dacd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/f3/4a/7479a14c575b5a2c968454d954ef38c11be783eec500d7855d\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=50ed11f3791606f2155eab3cc5f9be0644d88a9700cec5fc14e540cdbc418eae\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=5a1c99c986327d9b202bcdc31b383f3275f6f6c078822a3dc6251119fe17c194\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30052 sha256=cca24e5f1d30bc6f71a7eea9166667d382614d1438698ee6e8fe575458739819\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=714736c97fba0d057043edd7a54f6c0e357c1e6de2bf62d52b2ff37734a682c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\n",
            "Successfully built flash-attn optimum rouge-score deepspeed fire ffmpy wavedrom lit\n",
            "Installing collected packages: sentencepiece, pydub, ninja, nh3, lit, hjson, ffmpy, addict, websockets, urllib3, typing-extensions, svgwrite, smmap, shtab, shortuuid, setproctitle, semantic-version, querystring-parser, python-multipart, pynvml, orjson, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, markdown2, Mako, jmespath, humanfriendly, hf_transfer, h11, gunicorn, fire, einops, docstring-parser, docker-pycreds, dill, colorama, art, aioitertools, aiofiles, wavedrom, uvicorn, starlette, sentry-sdk, rouge-score, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, httpcore, gitdb, coloredlogs, botocore, tyro, tiktoken, responses, httpx, gitpython, fastapi, docker, databricks-cli, bitsandbytes, alembic, aiobotocore, wandb, s3fs, mlflow, gradio-client, fschat, datasets, transformers, gradio, evaluate, triton, torch, accelerate, xformers, trl, peft, optimum, bert-score, flash-attn, deepspeed, axolotl\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Running setup.py develop for axolotl\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.1 accelerate-0.26.1 addict-2.4.0 aiobotocore-2.5.4 aiofiles-23.2.1 aioitertools-0.11.0 alembic-1.13.1 art-6.1 axolotl bert-score-0.3.13 bitsandbytes-0.42.0 botocore-1.31.17 colorama-0.4.6 coloredlogs-15.0.1 databricks-cli-0.18.0 datasets-2.16.1 deepspeed-0.13.0 dill-0.3.7 docker-6.1.3 docker-pycreds-0.4.0 docstring-parser-0.15 einops-0.7.0 evaluate-0.4.0 fastapi-0.109.0 ffmpy-0.3.1 fire-0.5.0 flash-attn-2.3.3 fschat-0.2.34 gitdb-4.0.11 gitpython-3.1.41 gradio-3.50.2 gradio-client-0.6.1 gunicorn-21.2.0 h11-0.14.0 hf_transfer-0.1.4 hjson-3.1.0 httpcore-1.0.2 httpx-0.26.0 humanfriendly-10.0 jmespath-1.0.1 lit-17.0.6 markdown2-2.4.12 mlflow-2.9.2 multiprocess-0.70.15 nh3-0.2.15 ninja-1.11.1.1 numpy-1.26.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 optimum-1.13.2 orjson-3.9.12 peft-0.7.0 pydub-0.25.1 pynvml-11.5.0 python-multipart-0.0.6 querystring-parser-1.2.4 responses-0.18.0 rouge-score-0.1.2 s3fs-2023.6.0 semantic-version-2.10.0 sentencepiece-0.1.99 sentry-sdk-1.39.2 setproctitle-1.3.3 shortuuid-1.0.11 shtab-1.6.5 smmap-5.0.1 starlette-0.35.1 svgwrite-1.4.3 tiktoken-0.5.2 torch-2.0.1 transformers-4.37.0 triton-2.0.0 trl-0.7.10 typing-extensions-4.9.0 tyro-0.6.6 urllib3-1.26.18 uvicorn-0.27.0 wandb-0.16.2 wavedrom-2.0.3.post3 websockets-11.0.3 xformers-0.0.22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.train examples/mistral/qlora.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAVHIquxtj8P",
        "outputId": "2a760c21-6741-4fef-f810-12990b605069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-01-23 05:24:33.431258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 05:24:33.431308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 05:24:33.433167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 05:24:34.448703: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"/content/axolotl/src/axolotl/cli/__init__.py\", line 25, in <module>\n",
            "    from axolotl.common.cli import TrainerCliArgs, load_model_and_tokenizer\n",
            "  File \"/content/axolotl/src/axolotl/common/cli.py\", line 11, in <module>\n",
            "    from axolotl.utils.models import load_model, load_tokenizer\n",
            "  File \"/content/axolotl/src/axolotl/utils/models.py\", line 11, in <module>\n",
            "    from optimum.bettertransformer import BetterTransformer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/__init__.py\", line 14, in <module>\n",
            "    from .models import BetterTransformerManager\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/models/__init__.py\", line 17, in <module>\n",
            "    from .decoder_models import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/models/decoder_models.py\", line 18, in <module>\n",
            "    from transformers.models.bart.modeling_bart import BartAttention\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\", line 58, in <module>\n",
            "    from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flash_attn/__init__.py\", line 3, in <module>\n",
            "    from flash_attn.flash_attn_interface import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\", line 8, in <module>\n",
            "    import flash_attn_2_cuda as flash_attn_cuda\n",
            "ImportError: /usr/local/lib/python3.10/dist-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1023, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 643, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'axolotl.cli.train', 'examples/mistral/qlora.yml']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM3NnJU3xmfl",
        "outputId": "fea0c9b6-4ec9-4ea7-9fd0-3898b376b21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA Parameters:\n",
        "\n",
        "The lora_r parameter controls the rank of the update matrices. A lower rank results in smaller update matrices with fewer trainable parameters. This can help reduce overfitting and improve the generalization of the model. However, setting the rank too low can lead to underfitting and poor performance on the training data 1.\n",
        "\n",
        "The optimal value of lora_r depends on various factors such as the type of task, size of the training data, and the base model being used. In general, a higher value of lora_r is recommended for larger datasets and more complex models, while a lower value is recommended for smaller datasets and simpler models 2.\n",
        "\n",
        "For example, in the case of fine-tuning a BERT model on a small dataset, a lower value of lora_r may be more appropriate to prevent overfitting. On the other hand, for a large dataset with a complex model, a higher value of lora_r may be more suitable to achieve better performance 3."
      ],
      "metadata": {
        "id": "F4SwcLoW00CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.train examples/mistral/qlora.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgAlMB6-x54c",
        "outputId": "f0c1f853-45de-4da3-dcf0-cbff39f87aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-01-23 05:27:54.553561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 05:27:54.553621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 05:27:54.555544: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 05:27:55.618076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "[2024-01-23 05:27:57,876] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-01-23 05:28:10,836] [DEBUG] [axolotl.normalize_config:68] [PID:16214] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.54MB/s]\n",
            "[2024-01-23 05:28:11,424] [INFO] [axolotl.normalize_config:169] [PID:16214] [RANK:0] GPU memory usage baseline: 0.000GB (+0.441GB misc)\u001b[39m\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "\u001b[33m[2024-01-23 05:28:11,428] [WARNING] [axolotl.scripts.check_user_token:446] [PID:16214] [RANK:0] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.\u001b[39m\n",
            "tokenizer_config.json: 100% 967/967 [00:00<00:00, 5.66MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 20.2MB/s]\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 478kB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:01<00:00, 1.58MB/s]\n",
            "[2024-01-23 05:28:15,190] [DEBUG] [axolotl.load_tokenizer:215] [PID:16214] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:28:15,190] [DEBUG] [axolotl.load_tokenizer:216] [PID:16214] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-01-23 05:28:15,190] [DEBUG] [axolotl.load_tokenizer:217] [PID:16214] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:28:15,190] [DEBUG] [axolotl.load_tokenizer:218] [PID:16214] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-01-23 05:28:15,190] [INFO] [axolotl.load_tokenizer:223] [PID:16214] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-01-23 05:28:15,191] [INFO] [axolotl.load_tokenized_prepared_datasets:179] [PID:16214] [RANK:0] Unable to find prepared dataset in last_run_prepared/9bb367acfeb8b82e0b2722034757d2d0\u001b[39m\n",
            "[2024-01-23 05:28:15,191] [INFO] [axolotl.load_tokenized_prepared_datasets:180] [PID:16214] [RANK:0] Loading raw datasets...\u001b[39m\n",
            "\u001b[33m[2024-01-23 05:28:15,191] [WARNING] [axolotl.load_tokenized_prepared_datasets:182] [PID:16214] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
            "[2024-01-23 05:28:15,191] [INFO] [axolotl.load_tokenized_prepared_datasets:189] [PID:16214] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
            "Downloading readme: 100% 28.0/28.0 [00:00<00:00, 236kB/s]\n",
            "Downloading data: 100% 1.76M/1.76M [00:00<00:00, 6.44MB/s]\n",
            "Generating train split: 2000 examples [00:00, 53124.40 examples/s]\n",
            "Tokenizing Prompts (num_proc=12): 100% 2000/2000 [00:00<00:00, 2539.04 examples/s]\n",
            "[2024-01-23 05:28:24,683] [INFO] [axolotl.load_tokenized_prepared_datasets:392] [PID:16214] [RANK:0] merging datasets\u001b[39m\n",
            "[2024-01-23 05:28:24,687] [INFO] [axolotl.log:61] [PID:16214] [RANK:0] dropping attention_mask column\u001b[39m\n",
            "Dropping Long Sequences (num_proc=12): 100% 2000/2000 [00:00<00:00, 7974.89 examples/s]\n",
            "Add position_id column (Sample Packing) (num_proc=12): 100% 2000/2000 [00:00<00:00, 7282.68 examples/s]\n",
            "[2024-01-23 05:28:25,713] [INFO] [axolotl.load_tokenized_prepared_datasets:402] [PID:16214] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/9bb367acfeb8b82e0b2722034757d2d0\u001b[39m\n",
            "Saving the dataset (1/1 shards): 100% 2000/2000 [00:00<00:00, 109673.64 examples/s]\n",
            "[2024-01-23 05:28:25,747] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] total_num_tokens: 42325\u001b[39m\n",
            "[2024-01-23 05:28:25,750] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] `total_supervised_tokens: 29942`\u001b[39m\n",
            "[2024-01-23 05:28:31,868] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 42325\u001b[39m\n",
            "[2024-01-23 05:28:31,868] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] data_loader_len: 1\u001b[39m\n",
            "[2024-01-23 05:28:31,868] [INFO] [axolotl.log:61] [PID:16214] [RANK:0] sample_packing_eff_est across ranks: [0.8611043294270834]\u001b[39m\n",
            "[2024-01-23 05:28:31,868] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] sample_packing_eff_est: None\u001b[39m\n",
            "[2024-01-23 05:28:31,868] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] total_num_steps: 1\u001b[39m\n",
            "[2024-01-23 05:28:31,872] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] total_num_tokens: 385210\u001b[39m\n",
            "[2024-01-23 05:28:31,895] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] `total_supervised_tokens: 268140`\u001b[39m\n",
            "[2024-01-23 05:28:31,902] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 385210\u001b[39m\n",
            "[2024-01-23 05:28:31,902] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] data_loader_len: 22\u001b[39m\n",
            "[2024-01-23 05:28:31,902] [INFO] [axolotl.log:61] [PID:16214] [RANK:0] sample_packing_eff_est across ranks: [0.9796396891276041]\u001b[39m\n",
            "[2024-01-23 05:28:31,902] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] sample_packing_eff_est: 0.98\u001b[39m\n",
            "[2024-01-23 05:28:31,902] [DEBUG] [axolotl.log:61] [PID:16214] [RANK:0] total_num_steps: 22\u001b[39m\n",
            "[2024-01-23 05:28:31,902] [DEBUG] [axolotl.train.log:61] [PID:16214] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1\u001b[39m\n",
            "[2024-01-23 05:28:32,645] [DEBUG] [axolotl.load_tokenizer:215] [PID:16214] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:28:32,645] [DEBUG] [axolotl.load_tokenizer:216] [PID:16214] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-01-23 05:28:32,645] [DEBUG] [axolotl.load_tokenizer:217] [PID:16214] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:28:32,645] [DEBUG] [axolotl.load_tokenizer:218] [PID:16214] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-01-23 05:28:32,645] [INFO] [axolotl.load_tokenizer:223] [PID:16214] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-01-23 05:28:32,645] [DEBUG] [axolotl.train.log:61] [PID:16214] [RANK:0] loading model and peft_config...\u001b[39m\n",
            "[2024-01-23 05:28:32,920] [INFO] [axolotl.load_model:322] [PID:16214] [RANK:0] patching mistral with flash attention\u001b[39m\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 76.5MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/9.94G [00:00<00:51, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/9.94G [00:00<00:41, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 83.9M/9.94G [00:00<00:39, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 115M/9.94G [00:00<00:37, 262MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 157M/9.94G [00:00<00:31, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 199M/9.94G [00:00<00:30, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 241M/9.94G [00:00<00:35, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 283M/9.94G [00:01<00:33, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 325M/9.94G [00:01<00:32, 300MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 357M/9.94G [00:01<00:32, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 388M/9.94G [00:01<00:33, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 430M/9.94G [00:01<00:32, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 472M/9.94G [00:01<00:32, 296MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 514M/9.94G [00:01<00:31, 301MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 556M/9.94G [00:01<00:29, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 598M/9.94G [00:02<00:30, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 629M/9.94G [00:02<00:31, 295MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 682M/9.94G [00:02<00:27, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 724M/9.94G [00:02<00:26, 342MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 765M/9.94G [00:02<00:26, 345MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 807M/9.94G [00:02<00:25, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 849M/9.94G [00:02<00:26, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 891M/9.94G [00:02<00:31, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 933M/9.94G [00:03<00:30, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 975M/9.94G [00:03<00:28, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.02G/9.94G [00:03<00:26, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.07G/9.94G [00:03<00:24, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.11G/9.94G [00:03<00:25, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.15G/9.94G [00:03<00:27, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.20G/9.94G [00:03<00:26, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.24G/9.94G [00:03<00:26, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.28G/9.94G [00:04<00:25, 343MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.32G/9.94G [00:04<00:26, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.36G/9.94G [00:04<00:26, 327MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.41G/9.94G [00:04<00:26, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.45G/9.94G [00:04<00:26, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.49G/9.94G [00:04<00:26, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.53G/9.94G [00:04<00:24, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.57G/9.94G [00:04<00:24, 345MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.61G/9.94G [00:05<00:23, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.66G/9.94G [00:05<00:23, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.70G/9.94G [00:05<00:25, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.74G/9.94G [00:05<00:25, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.78G/9.94G [00:05<00:25, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.94G [00:05<00:23, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.88G/9.94G [00:05<00:24, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.92G/9.94G [00:06<00:24, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.94G [00:06<00:24, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.94G [00:06<00:24, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.04G/9.94G [00:06<00:23, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.94G [00:06<00:23, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.13G/9.94G [00:06<00:23, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.17G/9.94G [00:06<00:25, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.21G/9.94G [00:06<00:23, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.25G/9.94G [00:07<00:22, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.30G/9.94G [00:07<00:21, 355MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.34G/9.94G [00:07<00:21, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.38G/9.94G [00:07<00:23, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.42G/9.94G [00:07<00:21, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.46G/9.94G [00:07<00:20, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.52G/9.94G [00:07<00:19, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.56G/9.94G [00:07<00:21, 342MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.60G/9.94G [00:08<00:22, 331MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.64G/9.94G [00:08<00:22, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.68G/9.94G [00:08<00:24, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.72G/9.94G [00:08<00:25, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.76G/9.94G [00:08<00:23, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.80G/9.94G [00:08<00:21, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.85G/9.94G [00:08<00:19, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.89G/9.94G [00:08<00:22, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.94G/9.94G [00:09<00:21, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.98G/9.94G [00:09<00:20, 340MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.02G/9.94G [00:09<00:20, 345MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.06G/9.94G [00:09<00:20, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.94G [00:14<04:23, 25.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.15G/9.94G [00:14<03:10, 35.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.18G/9.94G [00:14<02:29, 45.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.21G/9.94G [00:14<01:56, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.24G/9.94G [00:15<01:31, 72.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.27G/9.94G [00:15<01:14, 89.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.94G [00:15<01:00, 109MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.34G/9.94G [00:15<00:46, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.39G/9.94G [00:15<00:37, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.43G/9.94G [00:15<00:31, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.46G/9.94G [00:15<00:30, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.50G/9.94G [00:16<00:27, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.53G/9.94G [00:16<00:26, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.58G/9.94G [00:16<00:23, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.61G/9.94G [00:16<00:22, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.64G/9.94G [00:16<00:22, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.67G/9.94G [00:16<00:21, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.70G/9.94G [00:16<00:23, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.73G/9.94G [00:16<00:22, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.76G/9.94G [00:17<00:26, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.94G [00:17<00:22, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.84G/9.94G [00:17<00:22, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.87G/9.94G [00:17<00:22, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.90G/9.94G [00:17<00:21, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.94G/9.94G [00:17<00:19, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.98G/9.94G [00:17<00:18, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.04G/9.94G [00:17<00:16, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.09G/9.94G [00:17<00:15, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.14G/9.94G [00:18<00:14, 411MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.18G/9.94G [00:18<00:14, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.23G/9.94G [00:18<00:15, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.27G/9.94G [00:18<00:16, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.31G/9.94G [00:18<00:17, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.35G/9.94G [00:18<00:16, 334MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.40G/9.94G [00:18<00:15, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.45G/9.94G [00:18<00:16, 343MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.94G [00:19<00:16, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.53G/9.94G [00:19<00:17, 301MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.57G/9.94G [00:19<00:16, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.62G/9.94G [00:19<00:15, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.67G/9.94G [00:19<00:21, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.70G/9.94G [00:19<00:20, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.74G/9.94G [00:20<00:19, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.78G/9.94G [00:20<00:18, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.82G/9.94G [00:20<00:16, 302MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.87G/9.94G [00:20<00:15, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.91G/9.94G [00:20<00:14, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.94G [00:20<00:15, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.99G/9.94G [00:20<00:15, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.03G/9.94G [00:20<00:15, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.08G/9.94G [00:21<00:16, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.11G/9.94G [00:21<00:17, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.14G/9.94G [00:21<00:17, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.17G/9.94G [00:21<00:16, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.21G/9.94G [00:21<00:15, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.25G/9.94G [00:21<00:14, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.30G/9.94G [00:21<00:15, 300MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.33G/9.94G [00:21<00:15, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.37G/9.94G [00:22<00:15, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.40G/9.94G [00:22<00:15, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.43G/9.94G [00:22<00:15, 295MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.46G/9.94G [00:22<00:15, 286MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.49G/9.94G [00:22<00:15, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.54G/9.94G [00:22<00:14, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.58G/9.94G [00:22<00:13, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.62G/9.94G [00:22<00:12, 342MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.66G/9.94G [00:22<00:12, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.71G/9.94G [00:23<00:11, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.76G/9.94G [00:23<00:11, 364MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.80G/9.94G [00:23<00:11, 367MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.84G/9.94G [00:23<00:13, 312MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.88G/9.94G [00:23<00:15, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.91G/9.94G [00:23<00:16, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.95G/9.94G [00:24<00:16, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.98G/9.94G [00:24<00:16, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.01G/9.94G [00:24<00:16, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.05G/9.94G [00:24<00:14, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.09G/9.94G [00:24<00:13, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.12G/9.94G [00:24<00:13, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.16G/9.94G [00:24<00:13, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.20G/9.94G [00:24<00:12, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.23G/9.94G [00:25<00:12, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.26G/9.94G [00:25<00:12, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.29G/9.94G [00:25<00:12, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.32G/9.94G [00:25<00:13, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.35G/9.94G [00:25<00:14, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.39G/9.94G [00:25<00:14, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.42G/9.94G [00:25<00:17, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.46G/9.94G [00:25<00:14, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.51G/9.94G [00:26<00:11, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.55G/9.94G [00:26<00:10, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.60G/9.94G [00:26<00:11, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.63G/9.94G [00:26<00:12, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.66G/9.94G [00:26<00:13, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.70G/9.94G [00:26<00:11, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.73G/9.94G [00:26<00:12, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.77G/9.94G [00:27<00:10, 296MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.81G/9.94G [00:27<00:11, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.84G/9.94G [00:27<00:11, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.87G/9.94G [00:27<00:11, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.90G/9.94G [00:27<00:11, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.93G/9.94G [00:27<00:13, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.96G/9.94G [00:27<00:14, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 7.00G/9.94G [00:28<00:12, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.04G/9.94G [00:28<00:12, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.08G/9.94G [00:28<00:11, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.11G/9.94G [00:28<00:11, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.14G/9.94G [00:28<00:12, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.17G/9.94G [00:28<00:12, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.20G/9.94G [00:28<00:12, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.25G/9.94G [00:29<00:10, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.29G/9.94G [00:29<00:09, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.32G/9.94G [00:29<00:10, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.35G/9.94G [00:29<00:09, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.38G/9.94G [00:29<00:09, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.42G/9.94G [00:29<00:08, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.47G/9.94G [00:29<00:08, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.50G/9.94G [00:29<00:08, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.53G/9.94G [00:30<00:08, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.56G/9.94G [00:30<00:09, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.60G/9.94G [00:30<00:08, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.64G/9.94G [00:30<00:07, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.69G/9.94G [00:30<00:07, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.72G/9.94G [00:30<00:07, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.76G/9.94G [00:30<00:07, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.80G/9.94G [00:30<00:07, 302MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.83G/9.94G [00:31<00:07, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.87G/9.94G [00:31<00:06, 298MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.91G/9.94G [00:31<00:07, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.95G/9.94G [00:31<00:06, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.99G/9.94G [00:31<00:06, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.03G/9.94G [00:31<00:05, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.08G/9.94G [00:31<00:04, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.13G/9.94G [00:31<00:04, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.17G/9.94G [00:31<00:04, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.21G/9.94G [00:32<00:05, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.25G/9.94G [00:32<00:05, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.28G/9.94G [00:32<00:05, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.32G/9.94G [00:32<00:05, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.36G/9.94G [00:32<00:05, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.39G/9.94G [00:32<00:05, 298MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.43G/9.94G [00:32<00:04, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.47G/9.94G [00:33<00:04, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.51G/9.94G [00:33<00:04, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.56G/9.94G [00:33<00:03, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.60G/9.94G [00:33<00:04, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.64G/9.94G [00:33<00:03, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.68G/9.94G [00:33<00:03, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.72G/9.94G [00:33<00:03, 342MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.77G/9.94G [00:33<00:03, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.81G/9.94G [00:34<00:03, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.85G/9.94G [00:34<00:03, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.89G/9.94G [00:34<00:03, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.93G/9.94G [00:34<00:03, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.97G/9.94G [00:34<00:03, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.00G/9.94G [00:34<00:03, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.03G/9.94G [00:34<00:03, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.06G/9.94G [00:35<00:03, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.09G/9.94G [00:35<00:03, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.12G/9.94G [00:35<00:03, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.15G/9.94G [00:35<00:03, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.19G/9.94G [00:35<00:03, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.22G/9.94G [00:35<00:02, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.25G/9.94G [00:35<00:02, 270MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.28G/9.94G [00:35<00:02, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.31G/9.94G [00:36<00:02, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.35G/9.94G [00:36<00:02, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.40G/9.94G [00:36<00:01, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.43G/9.94G [00:36<00:01, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.47G/9.94G [00:36<00:01, 312MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.51G/9.94G [00:36<00:01, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.55G/9.94G [00:36<00:01, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.60G/9.94G [00:36<00:00, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.65G/9.94G [00:37<00:00, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.69G/9.94G [00:37<00:00, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.73G/9.94G [00:37<00:00, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.77G/9.94G [00:37<00:00, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.83G/9.94G [00:37<00:00, 353MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.87G/9.94G [00:37<00:00, 345MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.94G/9.94G [00:37<00:00, 262MB/s]\n",
            "Downloading shards:  50% 1/2 [00:38<00:38, 38.17s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 21.0M/4.54G [00:00<00:43, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 52.4M/4.54G [00:00<00:25, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 94.4M/4.54G [00:00<00:18, 242MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 136M/4.54G [00:00<00:14, 294MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 178M/4.54G [00:00<00:14, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 220M/4.54G [00:00<00:13, 319MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 262M/4.54G [00:00<00:12, 345MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 304M/4.54G [00:01<00:14, 300MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 346M/4.54G [00:01<00:13, 307MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 388M/4.54G [00:01<00:12, 333MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 430M/4.54G [00:01<00:12, 325MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 472M/4.54G [00:01<00:13, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 514M/4.54G [00:01<00:12, 312MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 556M/4.54G [00:01<00:12, 321MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 598M/4.54G [00:01<00:12, 324MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 640M/4.54G [00:02<00:12, 324MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 682M/4.54G [00:02<00:11, 341MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 724M/4.54G [00:02<00:10, 349MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 765M/4.54G [00:02<00:10, 353MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 807M/4.54G [00:02<00:10, 356MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 849M/4.54G [00:02<00:10, 366MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 891M/4.54G [00:02<00:11, 325MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 933M/4.54G [00:02<00:10, 345MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 975M/4.54G [00:03<00:10, 347MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 1.02G/4.54G [00:03<00:10, 325MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.06G/4.54G [00:03<00:10, 338MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.10G/4.54G [00:03<00:10, 313MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.15G/4.54G [00:03<00:09, 349MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.20G/4.54G [00:03<00:11, 302MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.24G/4.54G [00:03<00:10, 316MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.28G/4.54G [00:04<00:10, 300MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.31G/4.54G [00:04<00:10, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.34G/4.54G [00:04<00:10, 293MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.38G/4.54G [00:04<00:10, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.42G/4.54G [00:04<00:10, 307MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.45G/4.54G [00:04<00:10, 295MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.49G/4.54G [00:04<00:09, 320MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.53G/4.54G [00:04<00:09, 303MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.56G/4.54G [00:04<00:10, 296MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.59G/4.54G [00:05<00:09, 299MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.63G/4.54G [00:05<00:09, 299MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.67G/4.54G [00:05<00:09, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.70G/4.54G [00:05<00:09, 302MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.74G/4.54G [00:05<00:08, 317MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.78G/4.54G [00:05<00:08, 311MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.81G/4.54G [00:05<00:09, 300MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.85G/4.54G [00:05<00:09, 278MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.89G/4.54G [00:06<00:09, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.92G/4.54G [00:06<00:09, 280MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.95G/4.54G [00:06<00:09, 286MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.98G/4.54G [00:06<00:08, 289MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.02G/4.54G [00:06<00:08, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.07G/4.54G [00:06<00:07, 319MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 2.11G/4.54G [00:06<00:07, 344MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.15G/4.54G [00:06<00:06, 358MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.20G/4.54G [00:06<00:06, 389MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.24G/4.54G [00:07<00:05, 388MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.29G/4.54G [00:07<00:06, 375MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.33G/4.54G [00:07<00:06, 317MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.37G/4.54G [00:07<00:12, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.41G/4.54G [00:08<00:10, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.44G/4.54G [00:08<00:10, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.47G/4.54G [00:08<00:09, 222MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.51G/4.54G [00:08<00:08, 236MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.54G/4.54G [00:08<00:08, 248MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.57G/4.54G [00:08<00:07, 248MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.60G/4.54G [00:08<00:08, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.63G/4.54G [00:08<00:07, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.66G/4.54G [00:09<00:08, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.69G/4.54G [00:09<00:08, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.73G/4.54G [00:09<00:07, 231MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.76G/4.54G [00:09<00:07, 250MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.80G/4.54G [00:09<00:06, 259MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.84G/4.54G [00:09<00:05, 286MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.87G/4.54G [00:09<00:06, 274MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.92G/4.54G [00:09<00:05, 286MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.96G/4.54G [00:10<00:05, 299MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.99G/4.54G [00:10<00:05, 289MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.02G/4.54G [00:10<00:07, 198MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.05G/4.54G [00:10<00:07, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.08G/4.54G [00:10<00:06, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.11G/4.54G [00:10<00:06, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.15G/4.54G [00:11<00:06, 199MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.18G/4.54G [00:11<00:06, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.21G/4.54G [00:11<00:05, 233MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.24G/4.54G [00:11<00:05, 226MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.27G/4.54G [00:11<00:05, 246MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.30G/4.54G [00:13<00:24, 51.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.34G/4.54G [00:13<00:16, 74.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.39G/4.54G [00:13<00:11, 102MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.42G/4.54G [00:13<00:09, 124MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.46G/4.54G [00:13<00:06, 158MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.49G/4.54G [00:13<00:05, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.52G/4.54G [00:14<00:05, 200MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.55G/4.54G [00:14<00:04, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.59G/4.54G [00:14<00:04, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.62G/4.54G [00:14<00:03, 237MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.66G/4.54G [00:14<00:03, 258MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.69G/4.54G [00:14<00:03, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.72G/4.54G [00:14<00:03, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.75G/4.54G [00:14<00:03, 223MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.79G/4.54G [00:15<00:03, 236MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.82G/4.54G [00:15<00:03, 233MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.85G/4.54G [00:15<00:02, 247MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.89G/4.54G [00:15<00:02, 289MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.92G/4.54G [00:15<00:02, 294MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.95G/4.54G [00:15<00:02, 287MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 3.98G/4.54G [00:15<00:01, 290MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 4.02G/4.54G [00:15<00:01, 294MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.05G/4.54G [00:16<00:02, 243MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.09G/4.54G [00:16<00:01, 281MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.12G/4.54G [00:16<00:01, 278MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.15G/4.54G [00:16<00:01, 252MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 4.18G/4.54G [00:16<00:01, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.22G/4.54G [00:16<00:01, 229MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.25G/4.54G [00:16<00:01, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.28G/4.54G [00:17<00:01, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.31G/4.54G [00:17<00:01, 200MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.33G/4.54G [00:17<00:01, 185MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.35G/4.54G [00:17<00:01, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.37G/4.54G [00:17<00:01, 156MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.39G/4.54G [00:17<00:01, 141MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.41G/4.54G [00:18<00:01, 114MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.44G/4.54G [00:18<00:00, 108MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.46G/4.54G [00:18<00:00, 107MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.49G/4.54G [00:18<00:00, 137MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.54G/4.54G [00:18<00:00, 240MB/s]\n",
            "Downloading shards: 100% 2/2 [00:57<00:00, 28.68s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:16<00:00,  8.49s/it]\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 743kB/s]\n",
            "[2024-01-23 05:29:51,295] [INFO] [axolotl.load_model:639] [PID:16214] [RANK:0] GPU memory usage after model load: 4.344GB (+0.132GB cache, +0.925GB misc)\u001b[39m\n",
            "[2024-01-23 05:29:51,305] [INFO] [axolotl.load_model:663] [PID:16214] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2024-01-23 05:29:51,311] [INFO] [axolotl.load_model:675] [PID:16214] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-01-23 05:29:51,316] [INFO] [axolotl.load_lora:782] [PID:16214] [RANK:0] found linear modules: ['q_proj', 'v_proj', 'gate_proj', 'down_proj', 'o_proj', 'up_proj', 'k_proj']\u001b[39m\n",
            "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n",
            "[2024-01-23 05:29:52,448] [INFO] [axolotl.load_model:707] [PID:16214] [RANK:0] GPU memory usage after adapters: 4.672GB (+0.929GB cache, +0.925GB misc)\u001b[39m\n",
            "[2024-01-23 05:29:52,459] [INFO] [axolotl.train.log:61] [PID:16214] [RANK:0] Pre-saving adapter config to ./qlora-out\u001b[39m\n",
            "[2024-01-23 05:29:52,463] [INFO] [axolotl.train.log:61] [PID:16214] [RANK:0] Starting trainer...\u001b[39m\n",
            "[2024-01-23 05:29:52,825] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 385210\u001b[39m\n",
            "[2024-01-23 05:29:52,827] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 385210\u001b[39m\n",
            "  0% 0/5 [00:00<?, ?it/s][2024-01-23 05:29:52,953] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 385210\u001b[39m\n",
            "{'loss': 0.9932, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
            " 20% 1/5 [00:23<01:32, 23.19s/it][2024-01-23 05:30:16,144] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "[2024-01-23 05:30:17,795] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "[2024-01-23 05:30:17,795] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[2024-01-23 05:30:19,456] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "2it [00:01,  1.20it/s]   \u001b[A[2024-01-23 05:30:21,140] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 0.9899232983589172, 'eval_runtime': 5.0575, 'eval_samples_per_second': 39.545, 'eval_steps_per_second': 19.773, 'epoch': 0.18}\n",
            " 20% 1/5 [00:28<01:32, 23.19s/it]\n",
            "3it [00:03,  1.19s/it]\u001b[A\n",
            "                      \u001b[A[2024-01-23 05:30:42,760] [INFO] [axolotl.callbacks.on_step_end:125] [PID:16214] [RANK:0] GPU memory usage while training: 4.845GB (+17.655GB cache, +0.952GB misc)\u001b[39m\n",
            "{'loss': 0.9946, 'learning_rate': 4e-05, 'epoch': 0.36}\n",
            " 40% 2/5 [00:49<01:15, 25.21s/it][2024-01-23 05:30:42,767] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "[2024-01-23 05:30:44,430] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "[2024-01-23 05:30:44,430] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[2024-01-23 05:30:46,098] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "2it [00:01,  1.20it/s]   \u001b[A[2024-01-23 05:30:47,791] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 0.9859831929206848, 'eval_runtime': 5.0871, 'eval_samples_per_second': 39.315, 'eval_steps_per_second': 19.658, 'epoch': 0.36}\n",
            " 40% 2/5 [00:54<01:15, 25.21s/it]\n",
            "3it [00:03,  1.19s/it]\u001b[A\n",
            "{'loss': 0.9563, 'learning_rate': 6e-05, 'epoch': 0.55}\n",
            "{'loss': 1.0069, 'learning_rate': 8e-05, 'epoch': 0.73}\n",
            " 80% 4/5 [01:38<00:24, 24.25s/it][2024-01-23 05:31:31,236] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "[2024-01-23 05:31:32,901] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "[2024-01-23 05:31:32,901] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A[2024-01-23 05:31:34,575] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "2it [00:01,  1.19it/s]   \u001b[A[2024-01-23 05:31:36,270] [INFO] [axolotl.utils.samplers.multipack._len_est:178] [PID:16214] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 42325\u001b[39m\n",
            "\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 0.9417174458503723, 'eval_runtime': 5.0966, 'eval_samples_per_second': 39.242, 'eval_steps_per_second': 19.621, 'epoch': 0.73}\n",
            " 80% 4/5 [01:43<00:24, 24.25s/it]\n",
            "3it [00:03,  1.19s/it]\u001b[A\n",
            "{'loss': 0.9192, 'learning_rate': 0.0001, 'epoch': 0.91}\n",
            "{'train_runtime': 126.1886, 'train_samples_per_second': 14.264, 'train_steps_per_second': 0.04, 'train_loss': 0.9740505814552307, 'epoch': 0.91}\n",
            "100% 5/5 [02:06<00:00, 25.23s/it]\n",
            "[2024-01-23 05:31:59,090] [INFO] [axolotl.train.log:61] [PID:16214] [RANK:0] Training Completed!!! Saving pre-trained model to ./qlora-out\u001b[39m\n",
            "(PeftModelForCausalLM(   (base_model): LoraModel(     (model): MistralForCausalLM(       (model): MistralModel(         (embed_tokens): Embedding(32000, 4096)         (layers): ModuleList(           (0-31): 32 x MistralDecoderLayer(             (self_attn): MistralAttention(               (q_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=1024, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=1024, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): MistralRotaryEmbedding()             )             (mlp): MistralMLP(               (gate_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=14336, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=14336, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=14336, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): MistralRMSNorm()             (post_attention_layernorm): MistralRMSNorm()           )         )         (norm): MistralRMSNorm()       )       (lm_head): Linear(in_features=4096, out_features=32000, bias=False)     )   ) ), LlamaTokenizer(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), })\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.inference examples/mistral/qlora.yml \\\n",
        "    --lora_model_dir=\"./qlora-out\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfmogwWGyfGg",
        "outputId": "f78bb023-fbe9-42c2-e61a-0b29777f7a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-01-23 05:35:11.104053: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 05:35:11.104110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 05:35:11.105981: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 05:35:12.145678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "[2024-01-23 05:35:13,856] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "[2024-01-23 05:35:22,426] [DEBUG] [axolotl.normalize_config:68] [PID:18242] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "[2024-01-23 05:35:22,773] [INFO] [axolotl.normalize_config:169] [PID:18242] [RANK:0] GPU memory usage baseline: 0.000GB (+0.441GB misc)\u001b[39m\n",
            "[2024-01-23 05:35:22,775] [INFO] [axolotl.common.cli.load_model_and_tokenizer:49] [PID:18242] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1\u001b[39m\n",
            "[2024-01-23 05:35:23,566] [DEBUG] [axolotl.load_tokenizer:215] [PID:18242] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:35:23,566] [DEBUG] [axolotl.load_tokenizer:216] [PID:18242] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-01-23 05:35:23,566] [DEBUG] [axolotl.load_tokenizer:217] [PID:18242] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:35:23,566] [DEBUG] [axolotl.load_tokenizer:218] [PID:18242] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-01-23 05:35:23,566] [INFO] [axolotl.load_tokenizer:223] [PID:18242] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-01-23 05:35:23,566] [INFO] [axolotl.common.cli.load_model_and_tokenizer:51] [PID:18242] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
            "Loading checkpoint shards: 100% 2/2 [00:17<00:00,  8.63s/it]\n",
            "[2024-01-23 05:35:44,131] [INFO] [axolotl.load_model:639] [PID:18242] [RANK:0] GPU memory usage after model load: 4.344GB (+0.132GB cache, +0.925GB misc)\u001b[39m\n",
            "[2024-01-23 05:35:44,145] [INFO] [axolotl.load_model:663] [PID:18242] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2024-01-23 05:35:44,150] [INFO] [axolotl.load_model:675] [PID:18242] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-01-23 05:35:44,155] [INFO] [axolotl.load_lora:782] [PID:18242] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'o_proj', 'v_proj', 'k_proj', 'up_proj', 'q_proj']\u001b[39m\n",
            "[2024-01-23 05:35:44,155] [DEBUG] [axolotl.load_lora:798] [PID:18242] [RANK:0] Loading pretained PEFT - LoRA\u001b[39m\n",
            "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n",
            "[2024-01-23 05:35:45,536] [INFO] [axolotl.load_model:707] [PID:18242] [RANK:0] GPU memory usage after adapters: 4.672GB (+1.078GB cache, +0.925GB misc)\u001b[39m\n",
            "================================================================================\n",
            "Give me an instruction (Ctrl + D to submit): \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/axolotl/src/axolotl/cli/inference.py\", line 36, in <module>\n",
            "    fire.Fire(do_cli)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/content/axolotl/src/axolotl/cli/inference.py\", line 32, in do_cli\n",
            "    do_inference(cfg=parsed_cfg, cli_args=parsed_cli_args)\n",
            "  File \"/content/axolotl/src/axolotl/cli/__init__.py\", line 119, in do_inference\n",
            "    instruction = get_multi_line_input()\n",
            "  File \"/content/axolotl/src/axolotl/cli/__init__.py\", line 66, in get_multi_line_input\n",
            "    for line in sys.stdin:\n",
            "KeyboardInterrupt\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1023, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 640, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1927, in _wait\n",
            "    def _wait(self, timeout):\n",
            "KeyboardInterrupt\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.inference examples/mistral/qlora.yml \\\n",
        "    --qlora_model_dir=\"./qlora-out\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JfTzqTg0Jpk",
        "outputId": "7a87ad04-fa38-45c9-bf9d-3be8f4fc3f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-01-23 05:39:11.061392: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 05:39:11.061445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 05:39:11.063317: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 05:39:12.093620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "[2024-01-23 05:39:13,809] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "[2024-01-23 05:39:22,563] [DEBUG] [axolotl.normalize_config:68] [PID:19269] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "[2024-01-23 05:39:22,910] [INFO] [axolotl.normalize_config:169] [PID:19269] [RANK:0] GPU memory usage baseline: 0.000GB (+0.441GB misc)\u001b[39m\n",
            "[2024-01-23 05:39:22,912] [INFO] [axolotl.common.cli.load_model_and_tokenizer:49] [PID:19269] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1\u001b[39m\n",
            "[2024-01-23 05:39:23,651] [DEBUG] [axolotl.load_tokenizer:215] [PID:19269] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:39:23,652] [DEBUG] [axolotl.load_tokenizer:216] [PID:19269] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-01-23 05:39:23,652] [DEBUG] [axolotl.load_tokenizer:217] [PID:19269] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:39:23,652] [DEBUG] [axolotl.load_tokenizer:218] [PID:19269] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-01-23 05:39:23,652] [INFO] [axolotl.load_tokenizer:223] [PID:19269] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-01-23 05:39:23,652] [INFO] [axolotl.common.cli.load_model_and_tokenizer:51] [PID:19269] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
            "Loading checkpoint shards: 100% 2/2 [00:16<00:00,  8.21s/it]\n",
            "[2024-01-23 05:39:43,381] [INFO] [axolotl.load_model:639] [PID:19269] [RANK:0] GPU memory usage after model load: 4.344GB (+0.132GB cache, +0.925GB misc)\u001b[39m\n",
            "[2024-01-23 05:39:43,391] [INFO] [axolotl.load_model:663] [PID:19269] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2024-01-23 05:39:43,396] [INFO] [axolotl.load_model:675] [PID:19269] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-01-23 05:39:43,400] [INFO] [axolotl.load_lora:782] [PID:19269] [RANK:0] found linear modules: ['q_proj', 'down_proj', 'up_proj', 'k_proj', 'o_proj', 'v_proj', 'gate_proj']\u001b[39m\n",
            "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n",
            "[2024-01-23 05:39:44,517] [INFO] [axolotl.load_model:707] [PID:19269] [RANK:0] GPU memory usage after adapters: 4.672GB (+0.929GB cache, +0.925GB misc)\u001b[39m\n",
            "================================================================================\n",
            "Give me an instruction (Ctrl + D to submit): \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/axolotl/src/axolotl/cli/inference.py\", line 36, in <module>\n",
            "    fire.Fire(do_cli)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/content/axolotl/src/axolotl/cli/inference.py\", line 32, in do_cli\n",
            "    do_inference(cfg=parsed_cfg, cli_args=parsed_cli_args)\n",
            "  File \"/content/axolotl/src/axolotl/cli/__init__.py\", line 119, in do_inference\n",
            "    instruction = get_multi_line_input()\n",
            "  File \"/content/axolotl/src/axolotl/cli/__init__.py\", line 66, in get_multi_line_input\n",
            "    for line in sys.stdin:\n",
            "KeyboardInterrupt\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1023, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 640, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1942, in _wait\n",
            "    (pid, sts) = self._try_wait(os.WNOHANG)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.inference examples/mistral/qlora.yml \\\n",
        "    --qlora_model_dir=\"./qlora-out\" --gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHJmz1jT1EU6",
        "outputId": "3e43a17b-9ad1-45b1-bd03-e005c62ad70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-01-23 05:47:01.711918: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 05:47:01.711966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 05:47:01.713815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 05:47:02.804540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "[2024-01-23 05:47:04,611] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "[2024-01-23 05:47:16,954] [DEBUG] [axolotl.normalize_config:68] [PID:21245] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "[2024-01-23 05:47:17,305] [INFO] [axolotl.normalize_config:169] [PID:21245] [RANK:0] GPU memory usage baseline: 0.000GB (+0.441GB misc)\u001b[39m\n",
            "[2024-01-23 05:47:17,306] [INFO] [axolotl.common.cli.load_model_and_tokenizer:49] [PID:21245] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1\u001b[39m\n",
            "[2024-01-23 05:47:18,043] [DEBUG] [axolotl.load_tokenizer:215] [PID:21245] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:47:18,043] [DEBUG] [axolotl.load_tokenizer:216] [PID:21245] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-01-23 05:47:18,043] [DEBUG] [axolotl.load_tokenizer:217] [PID:21245] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-01-23 05:47:18,043] [DEBUG] [axolotl.load_tokenizer:218] [PID:21245] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-01-23 05:47:18,043] [INFO] [axolotl.load_tokenizer:223] [PID:21245] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-01-23 05:47:18,043] [INFO] [axolotl.common.cli.load_model_and_tokenizer:51] [PID:21245] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
            "Loading checkpoint shards: 100% 2/2 [00:16<00:00,  8.38s/it]\n",
            "[2024-01-23 05:47:38,193] [INFO] [axolotl.load_model:639] [PID:21245] [RANK:0] GPU memory usage after model load: 4.344GB (+0.132GB cache, +0.925GB misc)\u001b[39m\n",
            "[2024-01-23 05:47:38,205] [INFO] [axolotl.load_model:663] [PID:21245] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2024-01-23 05:47:38,211] [INFO] [axolotl.load_model:675] [PID:21245] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-01-23 05:47:38,216] [INFO] [axolotl.load_lora:782] [PID:21245] [RANK:0] found linear modules: ['k_proj', 'gate_proj', 'up_proj', 'down_proj', 'o_proj', 'q_proj', 'v_proj']\u001b[39m\n",
            "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n",
            "[2024-01-23 05:47:39,364] [INFO] [axolotl.load_model:707] [PID:21245] [RANK:0] GPU memory usage after adapters: 4.672GB (+0.929GB cache, +0.925GB misc)\u001b[39m\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://7589aa0c8d0b2cfba9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1023, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 640, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1942, in _wait\n",
            "    (pid, sts) = self._try_wait(os.WNOHANG)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7589aa0c8d0b2cfba9.gradio.live\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8VgRHBZx23Kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}